{
  "model_info": {
    "name": "llama-2-7b-chat",
    "description": "LLaMA-2 7B Chat model optimized for command processing and conversational tasks",
    "task": "command_processing",
    "architecture": "LLaMA",
    "parameters": "7B",
    "context_length": 4096
  },
  "fine_tuning": {
    "framework": "transformers",
    "method": "LoRA",
    "dataset_format": "jsonl",
    "training_config": {
      "learning_rate": 2e-4,
      "batch_size": 4,
      "gradient_accumulation_steps": 4,
      "num_train_epochs": 3,
      "warmup_steps": 100,
      "logging_steps": 10,
      "save_steps": 500,
      "evaluation_strategy": "steps",
      "eval_steps": 500,
      "max_grad_norm": 1.0,
      "weight_decay": 0.01,
      "adam_epsilon": 1e-8,
      "seed": 42
    },
    "lora_config": {
      "r": 16,
      "lora_alpha": 32,
      "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
      "lora_dropout": 0.1,
      "bias": "none",
      "task_type": "CAUSAL_LM"
    }
  },
  "inference_config": {
    "temperature": 0.7,
    "max_tokens": 512,
    "top_k": 40,
    "top_p": 0.9,
    "repetition_penalty": 1.1,
    "do_sample": true,
    "pad_token_id": 0,
    "eos_token_id": 2
  },
  "prompt_templates": {
    "command_processing": "You are Mira, an AI assistant that takes commands, uses function tools, and responds with a user friendly message.\n\nUser: {input}\nAssistant:",
    "system_with_context": "You are Mira, an AI assistant that takes commands, uses function tools, and responds with a user friendly message.\n\nContext: {context}\n\nUser: {input}\nAssistant:",
    "chat_format": "<s>[INST] {instruction} [/INST] {response} </s>"
  },
  "dataset_requirements": {
    "command_tasks": [
      "weather_queries",
      "time_requests", 
      "system_control",
      "calendar_management",
      "reminder_setting",
      "general_conversation"
    ],
    "format": "conversational_jsonl",
    "min_samples": 1000,
    "max_length": 512
  }
}