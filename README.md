# ğŸ¤ Mira Backend

A real-time audio processing and AI conversation backend built with FastAPI, featuring speaker diarization, wake word detection, and multi-LLM support.

## âœ¨ Features

- **Real-time Audio Processing** - Live speech recognition with Whisper
- **Speaker Diarization** - Identify and track multiple speakers
- **Wake Word Detection** - Customizable wake words for voice activation
- **Multi-LLM Support** - Gemini, OpenAI, Anthropic, LM Studio, and AWS Bedrock
- **WebSocket Streaming** - Real-time audio streaming and responses
- **Database Integration** - SQLite (local) and PostgreSQL (production)
- **AWS Lambda Ready** - Serverless deployment with RDS

## ğŸš€ Quick Start

### Local Development

```bash
# 1. Clone and setup
git clone <your-repo>
cd mira-backend

# 2. Install dependencies
make install

# 3. Run tests
make test

# 4. Start development server
make dev
```

### AWS Lambda Deployment

```bash
# 1. Setup AWS environment
make setup-aws

# 2. Deploy to AWS Lambda
make deploy
```

## ğŸ“ Project Structure

```
mira-backend/
â”œâ”€â”€ app/                    # Main application code
â”‚   â”œâ”€â”€ api/               # FastAPI routers (v1 & v2)
â”‚   â”œâ”€â”€ core/              # Configuration and constants
â”‚   â”œâ”€â”€ db/                # Database models and session
â”‚   â”œâ”€â”€ models/            # SQLAlchemy models
â”‚   â”œâ”€â”€ services/          # Business logic services
â”‚   â””â”€â”€ tests/             # Test suite
â”œâ”€â”€ scripts/               # Deployment and migration scripts
â”œâ”€â”€ docs/                  # Documentation
â””â”€â”€ serverless.yml         # AWS Lambda configuration
```

## ğŸ› ï¸ Development

### Available Commands

```bash
make help          # Show all available commands
make test          # Run tests with coverage
make lint          # Run linting
make format        # Format code
make dev           # Start development server
make clean         # Clean up temporary files
```

### Configuration

- **Constants**: `app/core/constants.py` - Business logic constants
- **Settings**: `app/core/config.py` - Environment-dependent settings
- **Environment**: `.env` - Local environment variables

## ğŸ“š Documentation

- [Deployment Guide](docs/deployment.md) - AWS Lambda and RDS setup
- [Database Guide](docs/database.md) - Database configuration and migrations
- [API Reference](docs/api.md) - API endpoints and usage

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file with your credentials:

```bash
# LLM Backend (choose one)
GEMINI_API_KEY=your_gemini_key
OPENAI_API_KEY=your_openai_key
LLM_BACKEND=gemini

# Database
DATABASE_URL=sqlite:///./mira.db
```

### Supported LLM Backends

- **Gemini** - Google's Gemini API
- **OpenAI** - GPT models
- **Anthropic** - Claude models
- **LM Studio** - Local models
- **AWS Bedrock** - Amazon's managed models

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run specific test file
pytest app/tests/unit/test_services.py -v

# Run with coverage
make test-cov
```

## ğŸš€ Deployment

### AWS Lambda (Recommended)

```bash
# 1. Setup AWS environment
./scripts/setup-lambda.sh

# 2. Deploy to AWS
./scripts/deploy.sh

# 3. Run database migrations
./scripts/migrate-db.sh
```

### Docker (Alternative)

```bash
# Build and run with Docker
make docker-build
make docker-run
```

## ğŸ“– API Usage

### WebSocket Connection

```javascript
const ws = new WebSocket('ws://localhost:8000/ws/{network_id}');
ws.send(audioData); // Send PCM audio data
```

### REST Endpoints

- `POST /api/v1/interactions` - Process audio interactions
- `GET /api/v1/persons` - List speakers
- `GET /api/v1/conversations` - Get conversation history

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `make test`
5. Submit a pull request

## ğŸ“„ License

[Your License Here]

---

**Need help?** Check the [documentation](docs/) or open an issue.
